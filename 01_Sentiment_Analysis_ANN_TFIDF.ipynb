{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people will give panamaverdict in na electionw...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just bark dog where do panama expose corruptio...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It be clear from maryam nawaz bitter na campai...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nab references trigger by panama verdict be ha...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jaise motu gang saath hua tha he do not accept...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        category  \\\n",
       "0  people will give panamaverdict in na electionw...  panama_verdict   \n",
       "1  just bark dog where do panama expose corruptio...  panama_verdict   \n",
       "2  It be clear from maryam nawaz bitter na campai...  panama_verdict   \n",
       "3  nab references trigger by panama verdict be ha...  panama_verdict   \n",
       "4  jaise motu gang saath hua tha he do not accept...  panama_verdict   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          1  \n",
       "3          0  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = '../datasets/combined/data/sentiment/num_label_tweets.csv'\n",
    "my_df = pd.read_csv(csv)#,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 660645 entries, 0 to 660644\n",
      "Data columns (total 3 columns):\n",
      "text         660645 non-null object\n",
      "category     660645 non-null object\n",
      "sentiment    660645 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 15.1+ MB\n"
     ]
    }
   ],
   "source": [
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    390792\n",
       "1    161068\n",
       "0    108785\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236846</th>\n",
       "      <td>cpec game changer for region nawaz sharif duba...</td>\n",
       "      <td>cpec</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236847</th>\n",
       "      <td>good question china sponsor pakistan to keep t...</td>\n",
       "      <td>cpec</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236848</th>\n",
       "      <td>cpececo new ecomic zone change the world econo...</td>\n",
       "      <td>cpec</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236849</th>\n",
       "      <td>cpec be now already success those do terrorism...</td>\n",
       "      <td>cpec</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236850</th>\n",
       "      <td>score at cpec</td>\n",
       "      <td>cpec</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text category sentiment\n",
       "236846  cpec game changer for region nawaz sharif duba...     cpec  negative\n",
       "236847  good question china sponsor pakistan to keep t...     cpec  positive\n",
       "236848  cpececo new ecomic zone change the world econo...     cpec  positive\n",
       "236849  cpec be now already success those do terrorism...     cpec   neutral\n",
       "236850                                      score at cpec     cpec   neutral"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df[my_df.category == 'cpec'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600773</th>\n",
       "      <td>just play cardboard castle dengue fever the de...</td>\n",
       "      <td>dengue</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600774</th>\n",
       "      <td>big gigs for January dengue fever johnny cash ...</td>\n",
       "      <td>dengue</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600775</th>\n",
       "      <td>st paul dengue fever saturday and more prior a...</td>\n",
       "      <td>dengue</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600776</th>\n",
       "      <td>diferencias zica dengue chicungunya pictwitter...</td>\n",
       "      <td>dengue</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600777</th>\n",
       "      <td>eat more bacon no let up dengue mosquito larva...</td>\n",
       "      <td>dengue</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text category sentiment\n",
       "600773  just play cardboard castle dengue fever the de...   dengue   neutral\n",
       "600774  big gigs for January dengue fever johnny cash ...   dengue   neutral\n",
       "600775  st paul dengue fever saturday and more prior a...   dengue  positive\n",
       "600776  diferencias zica dengue chicungunya pictwitter...   dengue   neutral\n",
       "600777  eat more bacon no let up dengue mosquito larva...   dengue  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df[my_df.category == 'dengue'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['sentiment'] = my_df['sentiment'].map({'negative': 0, 'positive': 1, 'neutral': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    390792\n",
       "1    161068\n",
       "0    108785\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cats = my_df.target.values.categories\n",
    "di = dict(zip(cats,np.arange(len(cats))))\n",
    "target = np_utils.to_categorical(my_df.target.map(di))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "onehot = pd.get_dummies(my_df.target)\n",
    "target_labels = onehot.columns\n",
    "target = onehot.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people will give panamaverdict in na electionw...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just bark dog where do panama expose corruptio...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It be clear from maryam nawaz bitter na campai...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nab references trigger by panama verdict be ha...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jaise motu gang saath hua tha he do not accept...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        category  \\\n",
       "0  people will give panamaverdict in na electionw...  panama_verdict   \n",
       "1  just bark dog where do panama expose corruptio...  panama_verdict   \n",
       "2  It be clear from maryam nawaz bitter na campai...  panama_verdict   \n",
       "3  nab references trigger by panama verdict be ha...  panama_verdict   \n",
       "4  jaise motu gang saath hua tha he do not accept...  panama_verdict   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          1  \n",
       "3          0  \n",
       "4          2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.to_csv('../datasets/combined/data/sentiment/num_label_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people will give panamaverdict in na electionw...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just bark dog where do panama expose corruptio...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It be clear from maryam nawaz bitter na campai...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nab references trigger by panama verdict be ha...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jaise motu gang saath hua tha he do not accept...</td>\n",
       "      <td>panama_verdict</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        category  \\\n",
       "0  people will give panamaverdict in na electionw...  panama_verdict   \n",
       "1  just bark dog where do panama expose corruptio...  panama_verdict   \n",
       "2  It be clear from maryam nawaz bitter na campai...  panama_verdict   \n",
       "3  nab references trigger by panama verdict be ha...  panama_verdict   \n",
       "4  jaise motu gang saath hua tha he do not accept...  panama_verdict   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          1  \n",
       "3          0  \n",
       "4          2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = '../datasets/combined/data/sentiment/num_label_tweets.csv'\n",
    "my_df = pd.read_csv(csv)#,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "nb_classes =4\n",
    "x = []\n",
    "y_train = []\n",
    "\n",
    "with open('../datasets/combined/data/sentiment/num_label_tweets.csv', 'r') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        text = row[0]\n",
    "        target = row[2]\n",
    "        \n",
    "        x.append(text)\n",
    "        y_train.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopattackingjudiciary trump indialostpmlninvestme aassaa aa panamaverdict justicekhosa pakistan dramasharif hamdamstatusaa aac aa 2\n"
     ]
    }
   ],
   "source": [
    "print(x[10],y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "#from keras import np_utils\n",
    "#`x = my_df.text\n",
    "y = keras.utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7fede3685990>\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import keras\n",
    "y = keras.utils.to_categorical(y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aritificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN with Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec1 = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "tvec1.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "x_train_tfidf = tvec1.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "x_validation_tfidf = tvec1.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train1' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_tfidf, y_train1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf.score(x_validation_tfidf, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e9d2ed6dde5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.score(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bvec = CountVectorizer(max_features=80000,ngram_range=(1, 2))\n",
    "bvec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660645"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 647429, 647430, 647431])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.arange(np.shape(y_train)[0])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_batch = index[32*0:32*(0+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch1 = x_train_tfidf[index_batch,:].toarray()\n",
    "X_batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch1 = y_train[index_batch]\n",
    "y_batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[index_batch]\n",
    "        #y_batch = y_data#[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_arrays_from_file(path):\n",
    "    while 1:\n",
    "        f = open(path)\n",
    "        for line in f:\n",
    "            # create numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            x, y = process_line(line)\n",
    "            img = load_images(x)\n",
    "            yield (img, y)\n",
    "        f.close()\n",
    "\n",
    "model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
    "        samples_per_epoch=10000, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5059/5058 [==============================] - 462s - loss: 0.2562 - acc: 0.9109 - val_loss: 0.1446 - val_acc: 0.9546\n",
      "Epoch 2/5\n",
      "5059/5058 [==============================] - 456s - loss: 0.0867 - acc: 0.9734 - val_loss: 0.1323 - val_acc: 0.9600\n",
      "Epoch 3/5\n",
      "5059/5058 [==============================] - 459s - loss: 0.0455 - acc: 0.9873 - val_loss: 0.1382 - val_acc: 0.9616\n",
      "Epoch 4/5\n",
      "5059/5058 [==============================] - 461s - loss: 0.0221 - acc: 0.9944 - val_loss: 0.1583 - val_acc: 0.9628\n",
      "Epoch 5/5\n",
      "5059/5058 [==============================] - 464s - loss: 0.0102 - acc: 0.9976 - val_loss: 0.1818 - val_acc: 0.9600\n",
      "CPU times: user 22min 53s, sys: 17min 51s, total: 40min 44s\n",
      "Wall time: 38min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss= 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\"\"\"model.fit(x_train_tfidf, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    #validation_split=0.1,\n",
    "    validation_data=(x_validation_tfidf, y_validation),\n",
    "         # steps_per_epoch=x_train_tfidf.shape[0]/32,\n",
    "    shuffle=True)\"\"\"\n",
    "model.fit_generator(generator=batch_generator(x_train_tfidf, y_train, 128),\n",
    "                   epochs=5, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                6400064   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 6,400,324\n",
      "Trainable params: 6,400,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"../datasets/combined/models/sentiment/keras_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../datasets/combined/models/sentiment/keras_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/sentiment/keras_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"../datasets/combined/models/sentiment/keras_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model had the best validation accuracy after 2 epochs, and after that, it fails to generalize so validation accuracy slowly decreases, while training accuracy increases. But if you remember the result I got from logistic regression (train accuracy: 84.19%, validation accuracy: 82.91%), you can see that the above neural network failed to outperform logistic regression in terms of validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if normalizing inputs have any effect on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer().fit(x_train_tfidf)\n",
    "x_train_tfidf_norm = norm.transform(x_train_tfidf)\n",
    "x_validation_tfidf_norm = norm.transform(x_validation_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20233/20232 [==============================] - 623s - loss: 0.1120 - acc: 0.9582 - val_loss: 0.0685 - val_acc: 0.9768\n",
      "Epoch 2/5\n",
      "20233/20232 [==============================] - 624s - loss: 0.0462 - acc: 0.9848 - val_loss: 0.0638 - val_acc: 0.9799\n",
      "Epoch 3/5\n",
      "20233/20232 [==============================] - 601s - loss: 0.0297 - acc: 0.9908 - val_loss: 0.0672 - val_acc: 0.9796\n",
      "Epoch 4/5\n",
      "20233/20232 [==============================] - 601s - loss: 0.0204 - acc: 0.9941 - val_loss: 0.0728 - val_acc: 0.9804\n",
      "Epoch 5/5\n",
      "20233/20232 [==============================] - 601s - loss: 0.0140 - acc: 0.9961 - val_loss: 0.0804 - val_acc: 0.9796\n",
      "CPU times: user 58min 22s, sys: 10min 43s, total: 1h 9min 6s\n",
      "Wall time: 50min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_n = Sequential()\n",
    "model_n.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_n.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_n.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_n.fit_generator(generator=batch_generator(x_train_tfidf_norm, y_train, 32),\n",
    "                    epochs=5, validation_data=(x_validation_tfidf_norm, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf_norm.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_n.to_json()\n",
    "with open(\"../datasets/combined/models/sentiment/keras_model_n.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../datasets/combined/models/sentiment/keras_model_n.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/sentiment/keras_model_n.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_n = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_n.load_weights(\"../datasets/combined/models/sentiment/keras_model_n.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "29561/29560 [==============================] - 922s - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0027 - val_acc: 0.9987\n",
      "Epoch 2/5\n",
      "29561/29560 [==============================] - 927s - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0032 - val_acc: 0.9987\n",
      "Epoch 3/5\n",
      "29561/29560 [==============================] - 928s - loss: 0.0019 - acc: 0.9992 - val_loss: 0.0035 - val_acc: 0.9987\n",
      "Epoch 4/5\n",
      "29561/29560 [==============================] - 932s - loss: 0.0017 - acc: 0.9992 - val_loss: 0.0038 - val_acc: 0.9985\n",
      "Epoch 5/5\n",
      "29561/29560 [==============================] - 930s - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0041 - val_acc: 0.9985\n",
      "CPU times: user 1h 26min 47s, sys: 15min 55s, total: 1h 42min 42s\n",
      "Wall time: 1h 17min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit_generator(generator=batch_generator(x_train_tfidf, y_train, 32),\n",
    "                    epochs=5, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                6400064   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 6,400,454\n",
      "Trainable params: 6,400,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model1.summary())\n",
    "# serialize model to JSON\n",
    "model_json = model1.to_json()\n",
    "with open(\"../datasets/combined/models/sentiment/keras_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model1.save_weights(\"../datasets/combined/models/sentiment/keras_model1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/sentiment/keras_model1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model1 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model1.load_weights(\"../datasets/combined/models/sentiment/keras_model1.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_shuffle(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    np.random.shuffle(index)\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[index_batch]#[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            np.random.shuffle(index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "29561/29560 [==============================] - 910s - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0031 - val_acc: 0.9987\n",
      "Epoch 2/5\n",
      "29561/29560 [==============================] - 915s - loss: 0.0019 - acc: 0.9991 - val_loss: 0.0032 - val_acc: 0.9986\n",
      "Epoch 3/5\n",
      "29561/29560 [==============================] - 908s - loss: 0.0017 - acc: 0.9992 - val_loss: 0.0035 - val_acc: 0.9987\n",
      "Epoch 4/5\n",
      "29561/29560 [==============================] - 912s - loss: 0.0015 - acc: 0.9992 - val_loss: 0.0040 - val_acc: 0.9985\n",
      "Epoch 5/5\n",
      "29561/29560 [==============================] - 897s - loss: 0.0014 - acc: 0.9993 - val_loss: 0.0041 - val_acc: 0.9985\n",
      "CPU times: user 1h 25min 7s, sys: 15min 41s, total: 1h 40min 49s\n",
      "Wall time: 1h 15min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_s = Sequential()\n",
    "model_s.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_s.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_s.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n",
    "                    epochs=5, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model_s.to_json()\n",
    "with open(\"../datasets/combined/models/sentiment/keras_model_s.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_s.save_weights(\"../datasets/combined/models/sentiment/keras_model_s.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/sentiment/keras_model_s.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_s = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_s.load_weights(\"../datasets/combined/models/sentiment/keras_model_s.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "29561/29560 [==============================] - 925s - loss: 0.0107 - acc: 0.9967 - val_loss: 0.0030 - val_acc: 0.9987\n",
      "Epoch 2/5\n",
      "29561/29560 [==============================] - 968s - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0034 - val_acc: 0.9989\n",
      "Epoch 3/5\n",
      "29561/29560 [==============================] - 901s - loss: 0.0019 - acc: 0.9991 - val_loss: 0.0035 - val_acc: 0.9988\n",
      "Epoch 4/5\n",
      "29561/29560 [==============================] - 915s - loss: 0.0018 - acc: 0.9992 - val_loss: 0.0042 - val_acc: 0.9985\n",
      "Epoch 5/5\n",
      "29561/29560 [==============================] - 914s - loss: 0.0017 - acc: 0.9992 - val_loss: 0.0041 - val_acc: 0.9986\n",
      "CPU times: user 1h 27min 32s, sys: 15min 41s, total: 1h 43min 14s\n",
      "Wall time: 1h 17min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_s_1 = Sequential()\n",
    "model_s_1.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_s_1.add(Dropout(0.2))\n",
    "model_s_1.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_s_1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s_1.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n",
    "                    epochs=5, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model_s_1.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_s_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_s_1.save_weights(\"../datasets/combined/models/classification/keras_model_s_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/classification/keras_model_s_1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_s_1 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_s_1.load_weights(\"../datasets/combined/models/classification/keras_model_s_1.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "29561/29560 [==============================] - 901s - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0028 - val_acc: 0.9988\n",
      "Epoch 2/2\n",
      "29561/29560 [==============================] - 902s - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0037 - val_acc: 0.9986\n",
      "CPU times: user 33min 50s, sys: 6min 10s, total: 40min\n",
      "Wall time: 30min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "custom_adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_2 = Sequential()\n",
    "model_testing_2.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_2.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_testing_2.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n",
    "                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model_testing_2.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_testing_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_testing_2.save_weights(\"../datasets/combined/models/classification/keras_model_testing_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/classification/keras_model_testing_2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_testing_2 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_testing_2.load_weights(\"../datasets/combined/models/classification/keras_model_testing_2.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "29561/29560 [==============================] - 917s - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0036 - val_acc: 0.9985\n",
      "Epoch 2/2\n",
      "29561/29560 [==============================] - 918s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0039 - val_acc: 0.9987\n",
      "CPU times: user 34min 21s, sys: 6min 14s, total: 40min 36s\n",
      "Wall time: 30min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_3 = Sequential()\n",
    "model_testing_3.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_3.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_testing_3.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_3.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n",
    "                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model_testing_3.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_testing_3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_testing_3.save_weights(\"../datasets/combined/models/classification/keras_model_testing_3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/classification/keras_model_testing_3.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_testing_3 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_testing_3.load_weights(\"../datasets/combined/models/classification/keras_model_testing_3.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "29561/29560 [==============================] - 910s - loss: 0.0084 - acc: 0.9979 - val_loss: 0.0069 - val_acc: 0.9971\n",
      "Epoch 2/2\n",
      "29561/29560 [==============================] - 915s - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0058 - val_acc: 0.9980\n",
      "CPU times: user 34min 17s, sys: 6min 11s, total: 40min 29s\n",
      "Wall time: 30min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_4 = Sequential()\n",
    "model_testing_4.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_4.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_testing_4.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_4.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n",
    "                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 64)                6400064   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 6,400,454\n",
      "Trainable params: 6,400,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_testing_4.summary())\n",
    "# serialize model to JSON\n",
    "model_json = model_testing_4.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_testing_4.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_testing_4.save_weights(\"../datasets/combined/models/classification/keras_model_testing_4.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/classification/keras_model_testing_4.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_testing_4 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_testing_4.load_weights(\"../datasets/combined/models/classification/keras_model_testing_4.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "29561/29560 [==============================] - 915s - loss: 0.0149 - acc: 0.9955 - val_loss: 0.0034 - val_acc: 0.9986\n",
      "Epoch 2/5\n",
      "29561/29560 [==============================] - 917s - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0033 - val_acc: 0.9986\n",
      "Epoch 3/5\n",
      "29561/29560 [==============================] - 918s - loss: 0.0018 - acc: 0.9992 - val_loss: 0.0034 - val_acc: 0.9985\n",
      "Epoch 4/5\n",
      "29561/29560 [==============================] - 929s - loss: 0.0016 - acc: 0.9992 - val_loss: 0.0035 - val_acc: 0.9985\n",
      "Epoch 5/5\n",
      "29561/29560 [==============================] - 925s - loss: 0.0015 - acc: 0.9992 - val_loss: 0.0038 - val_acc: 0.9984\n",
      "CPU times: user 1h 26min 38s, sys: 15min 8s, total: 1h 41min 47s\n",
      "Wall time: 1h 16min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "custom_adam = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model_testing_5 = Sequential()\n",
    "model_testing_5.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model_testing_5.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_testing_5.compile(optimizer=custom_adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_testing_5.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n",
    "                    epochs=5, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 64)                6400064   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 6,400,454\n",
      "Trainable params: 6,400,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_testing_5.summary())\n",
    "# serialize model to JSON\n",
    "model_json = model_testing_5.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_testing_5.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_testing_5.save_weights(\"../datasets/combined/models/classification/keras_model_testing_5.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/classification/keras_model_testing_5.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_testing_5 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_testing_5.load_weights(\"../datasets/combined/models/classification/keras_model_testing_5.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "29561/29560 [==============================] - 1340s - loss: 0.0079 - acc: 0.9975 - val_loss: 0.0030 - val_acc: 0.9988\n",
      "Epoch 2/2\n",
      "29561/29560 [==============================] - 1313s - loss: 0.0019 - acc: 0.9991 - val_loss: 0.0031 - val_acc: 0.9987\n",
      "CPU times: user 41min 13s, sys: 11min 24s, total: 52min 37s\n",
      "Wall time: 44min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_s_2 = Sequential()\n",
    "model_s_2.add(Dense(128, activation='relu', input_dim=100000))\n",
    "model_s_2.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_s_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_s_2.fit_generator(generator=batch_generator_shuffle(x_train_tfidf, y_train, 32),\n",
    "                    epochs=2, validation_data=(x_validation_tfidf, y_validation),\n",
    "                    steps_per_epoch=x_train_tfidf.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               12800128  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 12,800,902\n",
      "Trainable params: 12,800,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_s_2.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_s_2.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_s_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_s_2.save_weights(\"../datasets/combined/models/classification/keras_model_s_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('../datasets/combined/models/classification/keras_model_s_2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_s_2 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model_s_2.load_weights(\"../datasets/combined/models/classification/keras_model_s_2.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model | learning rate | input layer (nodes) | data shuffling | hidden layer (nodes) | dropout | output layer (nodes) | training accuracy | validation accuracy |\n",
    "|-----|------|---|-----|----|----|----|----|\n",
    "| ANN_1 | 0.001 | 1 (100,000)  | X | 1 (64) relu  |  X  | 1 (1) softmax   | 99.82% | 99.53% |\n",
    "| ANN_2 | 0.001 | 1 (100,000)  | X | 1 (64) relu  |  0.2  | 1 (1) sigmoid   | 99.93% | 99.85% |\n",
    "| ANN_3 | 0.001 | 1 (100,000)  | O | 1 (64) relu  |  X  | 1 (1) sigmoid   | 99.93% | 99.85% |\n",
    "| ANN_4 | 0.001 | 1 (100,000)  | O | 1 (64) relu  |  0.2  | 1 (1) sigmoid   | 99.92% | 99.86%  |\n",
    "| ANN_5 | 0.0005 | 1 (100,000)  | O | 1 (64) relu  |  X  | 1 (1) sigmoid   | 99.91% | 99.87%  |\n",
    "| ANN_6 | 0.005 | 1 (100,000)  | O | 1 (64) relu  |  X  | 1 (1) sigmoid   | 99.86% | 99.87%  |\n",
    "| ANN_7 | 0.01 | 1 (100,000)  | O | 1 (64) relu  |  X  | 1 (1) sigmoid   | 99.86% | 99.80%  |\n",
    "| ANN_8 | 0.1 | 1 (100,000)  | O | 1 (64) relu  |  X  | 1 (1) sigmoid   | 99.92% | 99.84%  |\n",
    "| ANN_9 | 0.001 | 1 (100,000)  | O | 1 (128) relu  |  X  | 1 (1) sigmoid   | 99.91% | 99.87%  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
