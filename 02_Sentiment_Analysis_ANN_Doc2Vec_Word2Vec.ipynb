{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people will give panamaverdict in na election ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just barking dog where did panama expose corru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is clear from maryam nawaz bitter na campai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nab references triggered by panama verdict are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jaise motu gang saath hua tha he did not accep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  people will give panamaverdict in na election ...       1\n",
       "1  just barking dog where did panama expose corru...       1\n",
       "2  it is clear from maryam nawaz bitter na campai...       1\n",
       "3  nab references triggered by panama verdict are...       1\n",
       "4  jaise motu gang saath hua tha he did not accep...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = '../datasets/combined/data/all_tweets/num_label_tweets.csv'\n",
    "my_df = pd.read_csv(csv)#,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 965236 entries, 0 to 965235\n",
      "Data columns (total 2 columns):\n",
      "text      965236 non-null object\n",
      "target    965236 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 14.7+ MB\n"
     ]
    }
   ],
   "source": [
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_df.text\n",
    "y1 = my_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train1, y_validation_and_test1 = train_test_split(x, y1, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation1, y_test1 = train_test_split(x_validation_and_test, y_validation_and_test1, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/alizeb/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "model_ug_dbow = Doc2Vec.load('../datasets/combined/models/classification/d2v_model_ug_dbow.doc2vec')\n",
    "model_tg_dmm = Doc2Vec.load('../datasets/combined/models/classification/d2v_model_tg_dmm.doc2vec')\n",
    "model_ug_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_tg_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_ugdbow_tgdmm = get_concat_vectors(model_ug_dbow,model_tg_dmm, x_train, 200)\n",
    "validation_vecs_ugdbow_tgdmm = get_concat_vectors(model_ug_dbow,model_tg_dmm, x_validation, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 44s, sys: 532 ms, total: 3min 45s\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_ugdbow_tgdmm, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 512 ms, sys: 8 ms, total: 520 ms\n",
      "Wall time: 540 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9831520480880741"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.score(train_vecs_ugdbow_tgdmm, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 8.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9818690426854538"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.score(validation_vecs_ugdbow_tgdmm, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import csv\n",
    "nb_classes =6\n",
    "x = []\n",
    "y_train = []\n",
    "\n",
    "with open('../datasets/combined/data/all_tweets/num_label_tweets.csv', 'r') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        text = row[0]\n",
    "        target = row[1]\n",
    "        \n",
    "        x.append(text)\n",
    "        y_train.append(target)\n",
    "        \n",
    "y = keras.utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_01 = Sequential()\n",
    "model_d2v_01.add(Dense(64, activation='relu', input_dim=200))\n",
    "model_d2v_01.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_01.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_01.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                12864     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 13,254\n",
      "Trainable params: 13,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_01.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_01.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_01.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_01.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_01.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "84s - loss: 0.0147 - acc: 0.9949 - val_loss: 0.0114 - val_acc: 0.9957\n",
      "Epoch 2/10\n",
      "82s - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0106 - val_acc: 0.9961\n",
      "Epoch 3/10\n",
      "82s - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0098 - val_acc: 0.9967\n",
      "Epoch 4/10\n",
      "82s - loss: 0.0079 - acc: 0.9972 - val_loss: 0.0092 - val_acc: 0.9965\n",
      "Epoch 5/10\n",
      "82s - loss: 0.0075 - acc: 0.9973 - val_loss: 0.0094 - val_acc: 0.9967\n",
      "Epoch 6/10\n",
      "82s - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0093 - val_acc: 0.9965\n",
      "Epoch 7/10\n",
      "82s - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0107 - val_acc: 0.9962\n",
      "Epoch 8/10\n",
      "82s - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0098 - val_acc: 0.9964\n",
      "Epoch 9/10\n",
      "82s - loss: 0.0064 - acc: 0.9977 - val_loss: 0.0100 - val_acc: 0.9968\n",
      "Epoch 10/10\n",
      "82s - loss: 0.0062 - acc: 0.9977 - val_loss: 0.0102 - val_acc: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbdc5d3ddd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model_d2v_02 = Sequential()\n",
    "model_d2v_02.add(Dense(64, activation='relu', input_dim=200))\n",
    "model_d2v_02.add(Dense(64, activation='relu'))\n",
    "model_d2v_02.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_02.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_02.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                12864     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 17,414\n",
      "Trainable params: 17,414\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_02.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_02.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_02.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_02.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_02.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "96s - loss: 0.0150 - acc: 0.9948 - val_loss: 0.0113 - val_acc: 0.9956\n",
      "Epoch 2/10\n",
      "95s - loss: 0.0096 - acc: 0.9967 - val_loss: 0.0098 - val_acc: 0.9965\n",
      "Epoch 3/10\n",
      "92s - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0097 - val_acc: 0.9967\n",
      "Epoch 4/10\n",
      "92s - loss: 0.0080 - acc: 0.9972 - val_loss: 0.0098 - val_acc: 0.9965\n",
      "Epoch 5/10\n",
      "92s - loss: 0.0076 - acc: 0.9973 - val_loss: 0.0122 - val_acc: 0.9963\n",
      "Epoch 6/10\n",
      "92s - loss: 0.0073 - acc: 0.9974 - val_loss: 0.0110 - val_acc: 0.9962\n",
      "Epoch 7/10\n",
      "92s - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0102 - val_acc: 0.9967\n",
      "Epoch 8/10\n",
      "93s - loss: 0.0069 - acc: 0.9975 - val_loss: 0.0113 - val_acc: 0.9966\n",
      "Epoch 9/10\n",
      "92s - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0118 - val_acc: 0.9964\n",
      "Epoch 10/10\n",
      "97s - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0130 - val_acc: 0.9964\n",
      "CPU times: user 26min 39s, sys: 2min 8s, total: 28min 47s\n",
      "Wall time: 15min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_03 = Sequential()\n",
    "model_d2v_03.add(Dense(64, activation='relu', input_dim=200))\n",
    "model_d2v_03.add(Dense(64, activation='relu'))\n",
    "model_d2v_03.add(Dense(64, activation='relu'))\n",
    "model_d2v_03.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_03.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_03.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                12864     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 21,574\n",
      "Trainable params: 21,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_03.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_03.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_03.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_03.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_03.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "74s - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0111 - val_acc: 0.9958\n",
      "Epoch 2/10\n",
      "76s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0096 - val_acc: 0.9964\n",
      "Epoch 3/10\n",
      "75s - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0104 - val_acc: 0.9962\n",
      "Epoch 4/10\n",
      "74s - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0100 - val_acc: 0.9965\n",
      "Epoch 5/10\n",
      "76s - loss: 0.0074 - acc: 0.9975 - val_loss: 0.0100 - val_acc: 0.9966\n",
      "Epoch 6/10\n",
      "74s - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0104 - val_acc: 0.9964\n",
      "Epoch 7/10\n",
      "75s - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0111 - val_acc: 0.9962\n",
      "Epoch 8/10\n",
      "75s - loss: 0.0063 - acc: 0.9978 - val_loss: 0.0107 - val_acc: 0.9966\n",
      "Epoch 9/10\n",
      "73s - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0118 - val_acc: 0.9961\n",
      "Epoch 10/10\n",
      "78s - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0123 - val_acc: 0.9961\n",
      "CPU times: user 23min 14s, sys: 2min 4s, total: 25min 18s\n",
      "Wall time: 12min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_04 = Sequential()\n",
    "model_d2v_04.add(Dense(128, activation='relu', input_dim=200))\n",
    "model_d2v_04.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_04.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_04.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 26,502\n",
      "Trainable params: 26,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_04.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_04.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_04.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_04.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_04.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "85s - loss: 0.0136 - acc: 0.9953 - val_loss: 0.0117 - val_acc: 0.9958\n",
      "Epoch 2/10\n",
      "87s - loss: 0.0091 - acc: 0.9968 - val_loss: 0.0101 - val_acc: 0.9961\n",
      "Epoch 3/10\n",
      "84s - loss: 0.0080 - acc: 0.9972 - val_loss: 0.0092 - val_acc: 0.9970\n",
      "Epoch 4/10\n",
      "84s - loss: 0.0072 - acc: 0.9974 - val_loss: 0.0097 - val_acc: 0.9965\n",
      "Epoch 5/10\n",
      "84s - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0098 - val_acc: 0.9967\n",
      "Epoch 6/10\n",
      "84s - loss: 0.0061 - acc: 0.9978 - val_loss: 0.0101 - val_acc: 0.9965\n",
      "Epoch 7/10\n",
      "84s - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0114 - val_acc: 0.9963\n",
      "Epoch 8/10\n",
      "84s - loss: 0.0054 - acc: 0.9980 - val_loss: 0.0126 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      "84s - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0117 - val_acc: 0.9965\n",
      "Epoch 10/10\n",
      "84s - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0128 - val_acc: 0.9965\n",
      "CPU times: user 24min 51s, sys: 2min 12s, total: 27min 3s\n",
      "Wall time: 14min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_05 = Sequential()\n",
    "model_d2v_05.add(Dense(128, activation='relu', input_dim=200))\n",
    "model_d2v_05.add(Dense(128, activation='relu'))\n",
    "model_d2v_05.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_05.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_05.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 43,014\n",
      "Trainable params: 43,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_05.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_05.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_05.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_05.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_05.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "94s - loss: 0.0142 - acc: 0.9951 - val_loss: 0.0114 - val_acc: 0.9958\n",
      "Epoch 2/10\n",
      "94s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0093 - val_acc: 0.9963\n",
      "Epoch 3/10\n",
      "94s - loss: 0.0083 - acc: 0.9971 - val_loss: 0.0087 - val_acc: 0.9968\n",
      "Epoch 4/10\n",
      "94s - loss: 0.0076 - acc: 0.9973 - val_loss: 0.0124 - val_acc: 0.9960\n",
      "Epoch 5/10\n",
      "94s - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0102 - val_acc: 0.9965\n",
      "Epoch 6/10\n",
      "94s - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0099 - val_acc: 0.9964\n",
      "Epoch 7/10\n",
      "97s - loss: 0.0064 - acc: 0.9977 - val_loss: 0.0097 - val_acc: 0.9968\n",
      "Epoch 8/10\n",
      "94s - loss: 0.0061 - acc: 0.9978 - val_loss: 0.0117 - val_acc: 0.9966\n",
      "Epoch 9/10\n",
      "95s - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0127 - val_acc: 0.9964\n",
      "Epoch 10/10\n",
      "94s - loss: 0.0057 - acc: 0.9979 - val_loss: 0.0124 - val_acc: 0.9964\n",
      "CPU times: user 26min 46s, sys: 2min 15s, total: 29min 1s\n",
      "Wall time: 15min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_06 = Sequential()\n",
    "model_d2v_06.add(Dense(128, activation='relu', input_dim=200))\n",
    "model_d2v_06.add(Dense(128, activation='relu'))\n",
    "model_d2v_06.add(Dense(128, activation='relu'))\n",
    "model_d2v_06.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_06.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_06.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 59,526\n",
      "Trainable params: 59,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_06.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_06.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_06.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_06.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_06.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "75s - loss: 0.0139 - acc: 0.9953 - val_loss: 0.0109 - val_acc: 0.9960\n",
      "Epoch 2/10\n",
      "75s - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0097 - val_acc: 0.9966\n",
      "Epoch 3/10\n",
      "75s - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0104 - val_acc: 0.9963\n",
      "Epoch 4/10\n",
      "78s - loss: 0.0073 - acc: 0.9975 - val_loss: 0.0099 - val_acc: 0.9965\n",
      "Epoch 5/10\n",
      "75s - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0105 - val_acc: 0.9966\n",
      "Epoch 6/10\n",
      "75s - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0111 - val_acc: 0.9969\n",
      "Epoch 7/10\n",
      "75s - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0129 - val_acc: 0.9962\n",
      "Epoch 8/10\n",
      "75s - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0126 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      "75s - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0146 - val_acc: 0.9963\n",
      "Epoch 10/10\n",
      "75s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0145 - val_acc: 0.9962\n",
      "CPU times: user 23min 10s, sys: 2min 11s, total: 25min 21s\n",
      "Wall time: 12min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_07 = Sequential()\n",
    "model_d2v_07.add(Dense(256, activation='relu', input_dim=200))\n",
    "model_d2v_07.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_07.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_07.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               51456     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 52,998\n",
      "Trainable params: 52,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_07.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_07.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_07.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_07.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_07.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "91s - loss: 0.0132 - acc: 0.9954 - val_loss: 0.0110 - val_acc: 0.9959\n",
      "Epoch 2/10\n",
      "93s - loss: 0.0090 - acc: 0.9969 - val_loss: 0.0094 - val_acc: 0.9964\n",
      "Epoch 3/10\n",
      "91s - loss: 0.0078 - acc: 0.9973 - val_loss: 0.0098 - val_acc: 0.9965\n",
      "Epoch 4/10\n",
      "94s - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0097 - val_acc: 0.9967\n",
      "Epoch 5/10\n",
      "98s - loss: 0.0061 - acc: 0.9978 - val_loss: 0.0105 - val_acc: 0.9963\n",
      "Epoch 6/10\n",
      "94s - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0109 - val_acc: 0.9963\n",
      "Epoch 7/10\n",
      "94s - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0110 - val_acc: 0.9967\n",
      "Epoch 8/10\n",
      "95s - loss: 0.0048 - acc: 0.9982 - val_loss: 0.0128 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      "94s - loss: 0.0046 - acc: 0.9983 - val_loss: 0.0131 - val_acc: 0.9964\n",
      "Epoch 10/10\n",
      "91s - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0132 - val_acc: 0.9967\n",
      "CPU times: user 25min 53s, sys: 2min 43s, total: 28min 37s\n",
      "Wall time: 15min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_08 = Sequential()\n",
    "model_d2v_08.add(Dense(256, activation='relu', input_dim=200))\n",
    "model_d2v_08.add(Dense(256, activation='relu'))\n",
    "model_d2v_08.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_08.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_08.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 256)               51456     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 118,790\n",
      "Trainable params: 118,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_08.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_08.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_08.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_08.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_08.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "106s - loss: 0.0138 - acc: 0.9953 - val_loss: 0.0114 - val_acc: 0.9960\n",
      "Epoch 2/10\n",
      "106s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0104 - val_acc: 0.9961\n",
      "Epoch 3/10\n",
      "107s - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0095 - val_acc: 0.9968\n",
      "Epoch 4/10\n",
      "107s - loss: 0.0072 - acc: 0.9975 - val_loss: 0.0100 - val_acc: 0.9965\n",
      "Epoch 5/10\n",
      "107s - loss: 0.0065 - acc: 0.9977 - val_loss: 0.0110 - val_acc: 0.9965\n",
      "Epoch 6/10\n",
      "107s - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0109 - val_acc: 0.9962\n",
      "Epoch 7/10\n",
      "107s - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0097 - val_acc: 0.9969\n",
      "Epoch 8/10\n",
      "107s - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0097 - val_acc: 0.9969\n",
      "Epoch 9/10\n",
      "106s - loss: 0.0049 - acc: 0.9982 - val_loss: 0.0117 - val_acc: 0.9965\n",
      "Epoch 10/10\n",
      "107s - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0116 - val_acc: 0.9970\n",
      "CPU times: user 27min 45s, sys: 3min 7s, total: 30min 53s\n",
      "Wall time: 17min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_09 = Sequential()\n",
    "model_d2v_09.add(Dense(256, activation='relu', input_dim=200))\n",
    "model_d2v_09.add(Dense(256, activation='relu'))\n",
    "model_d2v_09.add(Dense(256, activation='relu'))\n",
    "model_d2v_09.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_09.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_09.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 256)               51456     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 184,582\n",
      "Trainable params: 184,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_09.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_09.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_09.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_09.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_09.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "82s - loss: 0.0134 - acc: 0.9955 - val_loss: 0.0111 - val_acc: 0.9959\n",
      "Epoch 2/10\n",
      "80s - loss: 0.0091 - acc: 0.9969 - val_loss: 0.0096 - val_acc: 0.9965\n",
      "Epoch 3/10\n",
      "81s - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0110 - val_acc: 0.9960\n",
      "Epoch 4/10\n",
      "81s - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0101 - val_acc: 0.9965\n",
      "Epoch 5/10\n",
      "80s - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0110 - val_acc: 0.9964\n",
      "Epoch 6/10\n",
      "81s - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0113 - val_acc: 0.9967\n",
      "Epoch 7/10\n",
      "80s - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0132 - val_acc: 0.9959\n",
      "Epoch 8/10\n",
      "81s - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0136 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      "81s - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0151 - val_acc: 0.9962\n",
      "Epoch 10/10\n",
      "81s - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0176 - val_acc: 0.9958\n",
      "CPU times: user 23min 42s, sys: 2min 35s, total: 26min 17s\n",
      "Wall time: 13min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_10 = Sequential()\n",
    "model_d2v_10.add(Dense(512, activation='relu', input_dim=200))\n",
    "model_d2v_10.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_10.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_10.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 512)               102912    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 105,990\n",
      "Trainable params: 105,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_10.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_10.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_10.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_10.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_10.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "116s - loss: 0.0132 - acc: 0.9955 - val_loss: 0.0112 - val_acc: 0.9961\n",
      "Epoch 2/10\n",
      "112s - loss: 0.0091 - acc: 0.9969 - val_loss: 0.0098 - val_acc: 0.9964\n",
      "Epoch 3/10\n",
      "113s - loss: 0.0078 - acc: 0.9973 - val_loss: 0.0102 - val_acc: 0.9965\n",
      "Epoch 4/10\n",
      "113s - loss: 0.0067 - acc: 0.9977 - val_loss: 0.0103 - val_acc: 0.9968\n",
      "Epoch 5/10\n",
      "113s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0116 - val_acc: 0.9964\n",
      "Epoch 6/10\n",
      "112s - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0114 - val_acc: 0.9964\n",
      "Epoch 7/10\n",
      "112s - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0142 - val_acc: 0.9962\n",
      "Epoch 8/10\n",
      "112s - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0150 - val_acc: 0.9964\n",
      "Epoch 9/10\n",
      "112s - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0158 - val_acc: 0.9964\n",
      "Epoch 10/10\n",
      "112s - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0159 - val_acc: 0.9966\n",
      "CPU times: user 27min 31s, sys: 4min 8s, total: 31min 40s\n",
      "Wall time: 18min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_11 = Sequential()\n",
    "model_d2v_11.add(Dense(512, activation='relu', input_dim=200))\n",
    "model_d2v_11.add(Dense(512, activation='relu'))\n",
    "model_d2v_11.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_11.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_11.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 512)               102912    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 368,646\n",
      "Trainable params: 368,646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_11.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_11.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_11.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_11.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_11.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/10\n",
      "136s - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0120 - val_acc: 0.9958\n",
      "Epoch 2/10\n",
      "135s - loss: 0.0094 - acc: 0.9969 - val_loss: 0.0097 - val_acc: 0.9962\n",
      "Epoch 3/10\n",
      "136s - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0116 - val_acc: 0.9962\n",
      "Epoch 4/10\n",
      "136s - loss: 0.0070 - acc: 0.9975 - val_loss: 0.0105 - val_acc: 0.9963\n",
      "Epoch 5/10\n",
      "136s - loss: 0.0061 - acc: 0.9978 - val_loss: 0.0119 - val_acc: 0.9965\n",
      "Epoch 6/10\n",
      "136s - loss: 0.0056 - acc: 0.9980 - val_loss: 0.0115 - val_acc: 0.9965\n",
      "Epoch 7/10\n",
      "136s - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0126 - val_acc: 0.9968\n",
      "Epoch 8/10\n",
      "136s - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0143 - val_acc: 0.9966\n",
      "Epoch 9/10\n",
      "136s - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0135 - val_acc: 0.9966\n",
      "Epoch 10/10\n",
      "140s - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0138 - val_acc: 0.9967\n",
      "CPU times: user 30min 40s, sys: 5min 6s, total: 35min 47s\n",
      "Wall time: 22min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "model_d2v_12 = Sequential()\n",
    "model_d2v_12.add(Dense(512, activation='relu', input_dim=200))\n",
    "model_d2v_12.add(Dense(512, activation='relu'))\n",
    "model_d2v_12.add(Dense(512, activation='relu'))\n",
    "model_d2v_12.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_12.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_12.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), epochs=10, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 512)               102912    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 631,302\n",
      "Trainable params: 631,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(model_d2v_12.summary())\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model_d2v_12.to_json()\n",
    "with open(\"../datasets/combined/models/classification/keras_model_d2v_12.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_d2v_12.save_weights(\"../datasets/combined/models/classification/keras_model_d2v_12.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model | input layer (nodes) | hidden layer (nodes) | output layer (nodes) | best validation accuracy | number of epochs for best validation accuracy |\n",
    "|-------|--------------|--------------|------------------|--------|--------|\n",
    "| model_d2v_01 | 1 (200)  | 1 (64) relu  |  1 (1) sigmoid   | 99.68% | epoch 7 |\n",
    "| model_d2v_02 | 1 (200)  | 2 (64) relu  |  1 (1) sigmoid   | 99.68% | epoch 9 |\n",
    "| model_d2v_03 | 1 (200)  | 3 (64) relu  |  1 (1) sigmoid   | 99.67% | epoch 3 |\n",
    "| model_d2v_04 | 1 (200)  | 1 (128) relu  |  1 (1) sigmoid   | 99.66% | epoch 5  |\n",
    "| model_d2v_05 | 1 (200)  | 2 (128) relu  |  1 (1) sigmoid   | 99.70% | epoch 3  |\n",
    "| model_d2v_06 | 1 (200)  | 3 (128) relu  |  1 (1) sigmoid   | 99.68% | epoch 7  |\n",
    "| model_d2v_07 | 1 (200)  | 1 (256) relu  |  1 (1) sigmoid   | 99.69% | epoch 6  |\n",
    "| model_d2v_08 | 1 (200)  | 2 (256) relu  |  1 (1) sigmoid   | 99.67% | epoch 10  |\n",
    "| model_d2v_09 | 1 (200)  | 3 (256) relu |  1 (1) sigmoid   | 99.70% | epoch 10  |\n",
    "| model_d2v_10 | 1 (200)  | 1 (512) relu |  1 (1) sigmoid   | 99.67% | epoch 6  |\n",
    "| model_d2v_11 | 1 (200)  | 2 (512) relu |  1 (1) sigmoid   | 99.68% | epoch 4  |\n",
    "| model_d2v_12 | 1 (200)  | 3 (512) relu |  1 (1) sigmoid   | 99.68% | epoch 7  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.99599, saving model to ../datasets/combined/models/classification/d2v_09_best_weights.00-0.9960.hdf5\n",
      "107s - loss: 0.0138 - acc: 0.9953 - val_loss: 0.0109 - val_acc: 0.9960\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.99599 to 0.99636, saving model to ../datasets/combined/models/classification/d2v_09_best_weights.01-0.9964.hdf5\n",
      "107s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0096 - val_acc: 0.9964\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.99636 to 0.99646, saving model to ../datasets/combined/models/classification/d2v_09_best_weights.02-0.9965.hdf5\n",
      "107s - loss: 0.0081 - acc: 0.9972 - val_loss: 0.0099 - val_acc: 0.9965\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc did not improve\n",
      "107s - loss: 0.0072 - acc: 0.9975 - val_loss: 0.0100 - val_acc: 0.9964\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc improved from 0.99646 to 0.99663, saving model to ../datasets/combined/models/classification/d2v_09_best_weights.04-0.9966.hdf5\n",
      "107s - loss: 0.0064 - acc: 0.9977 - val_loss: 0.0111 - val_acc: 0.9966\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc did not improve\n",
      "107s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0129 - val_acc: 0.9961\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc did not improve\n",
      "108s - loss: 0.0056 - acc: 0.9980 - val_loss: 0.0120 - val_acc: 0.9966\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc improved from 0.99663 to 0.99674, saving model to ../datasets/combined/models/classification/d2v_09_best_weights.07-0.9967.hdf5\n",
      "107s - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0116 - val_acc: 0.9967\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc did not improve\n",
      "107s - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0152 - val_acc: 0.9965\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc did not improve\n",
      "107s - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0137 - val_acc: 0.9966\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "107s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0143 - val_acc: 0.9967\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc did not improve\n",
      "107s - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0157 - val_acc: 0.9967\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc did not improve\n",
      "108s - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0154 - val_acc: 0.9963\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc did not improve\n",
      "108s - loss: 0.0040 - acc: 0.9985 - val_loss: 0.0178 - val_acc: 0.9962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd47e4d828>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "filepath=\"../datasets/combined/models/classification/d2v_09_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=5, mode='max') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "np.random.seed(seed)\n",
    "model_d2v_09_es = Sequential()\n",
    "model_d2v_09_es.add(Dense(256, activation='relu', input_dim=200))\n",
    "model_d2v_09_es.add(Dense(256, activation='relu'))\n",
    "model_d2v_09_es.add(Dense(256, activation='relu'))\n",
    "model_d2v_09_es.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_d2v_09_es.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_09_es.fit(train_vecs_ugdbow_tgdmm, y_train, validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), \n",
    "                    epochs=100, batch_size=32, verbose=2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I evaluate the model I just run, it will give me the result as same as I got from the last epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8736/9652 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01781654689798304, 0.9962356783174846]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d2v_09_es.evaluate(x=validation_vecs_ugdbow_tgdmm, y=y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('../datasets/combined/models/classification/d2v_09_best_weights.07-0.9967.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9376/9652 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.011632945165446261, 0.9967364375639693]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(x=validation_vecs_ugdbow_tgdmm, y=y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors extracted from Doc2Vec models (Average/Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/alizeb/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "model_ug_dmm = Doc2Vec.load('../datasets/combined/models/classification/d2v_model_ug_dmm.doc2vec')\n",
    "model_ug_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_ugdbowdmm(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_dbow[word],model_ug_dmm[word]).reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_ugdbowdmm_sum(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_dbow[word],model_ug_dmm[word]).reshape((1, size))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_dbowdmm = np.concatenate([get_w2v_ugdbowdmm(z, 200) for z in x_train])\n",
    "validation_vecs_w2v_dbowdmm = np.concatenate([get_w2v_ugdbowdmm(z, 200) for z in x_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 59s, sys: 4.66 s, total: 8min 4s\n",
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_dbowdmm, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9725445503522586"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_w2v_dbowdmm, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_dbowdmm_s = scale(train_vecs_w2v_dbowdmm)\n",
    "validation_vecs_w2v_dbowdmm_s = scale(validation_vecs_w2v_dbowdmm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_dbowdmm_s, y_train1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf.score(validation_vecs_w2v_dbowdmm_s, y_validation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how summed word vectors perform compared to the averaged counter part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_dbowdmm_sum = np.concatenate([get_w2v_ugdbowdmm_sum(z, 200) for z in x_train])\n",
    "validation_vecs_w2v_dbowdmm_sum = np.concatenate([get_w2v_ugdbowdmm_sum(z, 200) for z in x_validation])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_dbowdmm_sum, y_train1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf.score(validation_vecs_w2v_dbowdmm_sum, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_dbowdmm_sum_s = scale(train_vecs_w2v_dbowdmm_sum)\n",
    "validation_vecs_w2v_dbowdmm_sum_s = scale(validation_vecs_w2v_dbowdmm_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 48.4 s, total: 2min 51s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_dbowdmm_sum_s, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725250626566416"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_w2v_dbowdmm_sum_s, y_validation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors extracted from Doc2Vec models with TFIDF weighting (Average/Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 108244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(min_df=2)\n",
    "tvec.fit_transform(x_train)\n",
    "tfidf = dict(zip(tvec.get_feature_names(), tvec.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108244"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(model_ug_dbow.wv.vocab.keys()) & set(tvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 32min 8s, sys: 51.3 s, total: 3h 32min 59s\n",
      "Wall time: 3h 31min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_tfidf = {}\n",
    "for w in model_ug_dbow.wv.vocab.keys():\n",
    "    if w in tvec.get_feature_names():\n",
    "        w2v_tfidf[w] = np.append(model_ug_dbow[w],model_ug_dmm[w]) * tfidf[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/w2v_tfidf.p', 'wb') as fp:\n",
    "    pickle.dump(w2v_tfidf, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/w2v_tfidf.p', 'rb') as fp:\n",
    "    w2v_tfidf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 3.71 s, total: 1min 6s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_w2v_tfidf_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_tfidf, 'mean') for z in x_train]))\n",
    "validation_vecs_w2v_tfidf_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_tfidf, 'mean') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 24s, sys: 3.23 s, total: 4min 27s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_tfidf_mean, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9504765851636966"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_w2v_tfidf_mean, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 3.64 s, total: 1min 4s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_w2v_tfidf_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_tfidf, 'sum') for z in x_train]))\n",
    "validation_vecs_w2v_tfidf_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_tfidf, 'sum') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 12s, sys: 3.9 s, total: 5min 16s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_tfidf_sum, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487152921674264"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_w2v_tfidf_sum, y_validation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors extracted from Doc2Vec models with custom weighting (Average/Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer(max_features=100000)\n",
    "cvec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "verdict_train = x_train[y_train1 == 1]\n",
    "panama_train = x_train[y_train1 == 2]\n",
    "loadshedding_train = x_train[y_train1 == 3]\n",
    "cpec_train = x_train[y_train1 == 4]\n",
    "dengue_train = x_train[y_train1 == 5]\n",
    "panama_doc_matrix = cvec.transform(panama_train)\n",
    "verdict_doc_matrix = cvec.transform(verdict_train)\n",
    "loadshedding_doc_matrix = cvec.transform(loadshedding_train)\n",
    "cpec_doc_matrix = cvec.transform(cpec_train)\n",
    "dengue_doc_matrix = cvec.transform(dengue_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 0 ns, total: 56 ms\n",
      "Wall time: 53.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "panama_tf = np.sum(panama_doc_matrix,axis=0)\n",
    "verdict_tf = np.sum(verdict_doc_matrix,axis=0)\n",
    "loadshedding_tf = np.sum(loadshedding_doc_matrix,axis=0)\n",
    "cpec_tf = np.sum(cpec_doc_matrix,axis=0)\n",
    "dengue_tf = np.sum(dengue_doc_matrix,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verdict</th>\n",
       "      <th>panama</th>\n",
       "      <th>loadshedding</th>\n",
       "      <th>cpec</th>\n",
       "      <th>dengue</th>\n",
       "      <th>total</th>\n",
       "      <th>panama_rate</th>\n",
       "      <th>panama_freq_pct</th>\n",
       "      <th>panama_rate_normcdf</th>\n",
       "      <th>panama_freq_pct_normcdf</th>\n",
       "      <th>panama_normcdf_hmean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>livenews</th>\n",
       "      <td>0</td>\n",
       "      <td>11556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.981120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liveleak</th>\n",
       "      <td>0</td>\n",
       "      <td>21577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21578</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>0.981114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fonseca</th>\n",
       "      <td>1</td>\n",
       "      <td>3835</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3837</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.981054</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.990434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mossack</th>\n",
       "      <td>1</td>\n",
       "      <td>3820</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3822</td>\n",
       "      <td>0.999477</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.981053</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.990434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cdnpoli</th>\n",
       "      <td>2</td>\n",
       "      <td>3714</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3718</td>\n",
       "      <td>0.998924</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.980982</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.990397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iceland</th>\n",
       "      <td>0</td>\n",
       "      <td>7543</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7553</td>\n",
       "      <td>0.998676</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.980950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinton</th>\n",
       "      <td>0</td>\n",
       "      <td>3632</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3647</td>\n",
       "      <td>0.995887</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.980588</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.990194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0</td>\n",
       "      <td>6688</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>6720</td>\n",
       "      <td>0.995238</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.980503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offshore</th>\n",
       "      <td>7</td>\n",
       "      <td>13046</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>13111</td>\n",
       "      <td>0.995042</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>0.980477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>panamapapers</th>\n",
       "      <td>617</td>\n",
       "      <td>129682</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>130397</td>\n",
       "      <td>0.994517</td>\n",
       "      <td>0.045029</td>\n",
       "      <td>0.980408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              verdict  panama  loadshedding  cpec  dengue   total  \\\n",
       "livenews            0   11556             0     0       0   11556   \n",
       "liveleak            0   21577             0     0       1   21578   \n",
       "fonseca             1    3835             0     1       0    3837   \n",
       "mossack             1    3820             0     1       0    3822   \n",
       "cdnpoli             2    3714             0     1       1    3718   \n",
       "iceland             0    7543             1     8       1    7553   \n",
       "clinton             0    3632             2    10       3    3647   \n",
       "adult               0    6688            13     0      19    6720   \n",
       "offshore            7   13046            12    42       4   13111   \n",
       "panamapapers      617  129682            21    77       0  130397   \n",
       "\n",
       "              panama_rate  panama_freq_pct  panama_rate_normcdf  \\\n",
       "livenews         1.000000         0.004013             0.981120   \n",
       "liveleak         0.999954         0.007492             0.981114   \n",
       "fonseca          0.999479         0.001332             0.981054   \n",
       "mossack          0.999477         0.001326             0.981053   \n",
       "cdnpoli          0.998924         0.001290             0.980982   \n",
       "iceland          0.998676         0.002619             0.980950   \n",
       "clinton          0.995887         0.001261             0.980588   \n",
       "adult            0.995238         0.002322             0.980503   \n",
       "offshore         0.995042         0.004530             0.980477   \n",
       "panamapapers     0.994517         0.045029             0.980408   \n",
       "\n",
       "              panama_freq_pct_normcdf  panama_normcdf_hmean  \n",
       "livenews                     1.000000              0.990470  \n",
       "liveleak                     1.000000              0.990467  \n",
       "fonseca                      0.999996              0.990434  \n",
       "mossack                      0.999996              0.990434  \n",
       "cdnpoli                      0.999993              0.990397  \n",
       "iceland                      1.000000              0.990384  \n",
       "clinton                      0.999989              0.990194  \n",
       "adult                        1.000000              0.990156  \n",
       "offshore                     1.000000              0.990142  \n",
       "panamapapers                 1.000000              0.990107  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import hmean\n",
    "from scipy.stats import norm\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "\n",
    "panama = np.squeeze(np.asarray(panama_tf))\n",
    "verdict = np.squeeze(np.asarray(verdict_tf))\n",
    "loadshedding = np.squeeze(np.asarray(loadshedding_tf))\n",
    "cpec = np.squeeze(np.asarray(cpec_tf))\n",
    "dengue = np.squeeze(np.asarray(dengue_tf))\n",
    "\n",
    "term_freq_df2 = pd.DataFrame([verdict,panama,loadshedding,cpec,dengue],columns=cvec.get_feature_names()).transpose()\n",
    "term_freq_df2.columns = ['verdict', 'panama', 'loadshedding', 'cpec','dengue']\n",
    "term_freq_df2['total'] = term_freq_df2['verdict'] + term_freq_df2['panama'] + term_freq_df2['loadshedding'] + term_freq_df2['cpec'] + term_freq_df2['dengue']\n",
    "term_freq_df2['panama_rate'] = term_freq_df2['panama'] * 1./term_freq_df2['total']\n",
    "term_freq_df2['panama_freq_pct'] = term_freq_df2['panama'] * 1./term_freq_df2['panama'].sum()\n",
    "term_freq_df2['panama_rate_normcdf'] = normcdf(term_freq_df2['panama_rate'])\n",
    "term_freq_df2['panama_freq_pct_normcdf'] = normcdf(term_freq_df2['panama_freq_pct'])\n",
    "term_freq_df2['panama_normcdf_hmean'] = hmean([term_freq_df2['panama_rate_normcdf'], term_freq_df2['panama_freq_pct_normcdf']])\n",
    "term_freq_df2.sort_values(by='panama_normcdf_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df2.to_csv(\"../datasets/combined/data/term_freq_df2_panama.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.stats import hmean\n",
    "from scipy.stats import norm\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "term_freq_df2 = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()\n",
    "term_freq_df2.columns = ['negative', 'positive']\n",
    "term_freq_df2['total'] = term_freq_df2['negative'] + term_freq_df2['positive']\n",
    "term_freq_df2['pos_rate'] = term_freq_df2['positive'] * 1./term_freq_df2['total']\n",
    "term_freq_df2['pos_freq_pct'] = term_freq_df2['positive'] * 1./term_freq_df2['positive'].sum()\n",
    "term_freq_df2['pos_rate_normcdf'] = normcdf(term_freq_df2['pos_rate'])\n",
    "term_freq_df2['pos_freq_pct_normcdf'] = normcdf(term_freq_df2['pos_freq_pct'])\n",
    "term_freq_df2['pos_normcdf_hmean'] = hmean([term_freq_df2['pos_rate_normcdf'], term_freq_df2['pos_freq_pct_normcdf']])\n",
    "term_freq_df2.sort_values(by='pos_normcdf_hmean', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "panama_hmean = term_freq_df2.panama_normcdf_hmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.57 s, sys: 52 ms, total: 3.62 s\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_panama_hmean = {}\n",
    "for w in model_ug_dbow.wv.vocab.keys():\n",
    "    if w in panama_hmean.keys():\n",
    "        w2v_panama_hmean[w] = np.append(model_ug_dbow[w],model_ug_dmm[w]) * panama_hmean[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/combined/models/classification/w2v_hmean.p', 'wb') as fp:\n",
    "    pickle.dump(w2v_panama_hmean, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/w2v_hmean.p', 'rb') as fp:\n",
    "    w2v_panama_hmean = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_panamahmean_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_panama_hmean, 'mean') for z in x_train]))\n",
    "validation_vecs_w2v_panamahmean_mean = scale(np.concatenate([get_w2v_general(z, 200, w2v_panama_hmean, 'mean') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 20s, sys: 5.06 s, total: 7min 25s\n",
      "Wall time: 7min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_panamahmean_mean, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9872565271446332"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_w2v_panamahmean_mean, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_panamahmean_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_panama_hmean, 'sum') for z in x_train]))\n",
    "validation_vecs_w2v_panamahmean_sum = scale(np.concatenate([get_w2v_general(z, 200, w2v_panama_hmean, 'sum') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 3s, sys: 5.39 s, total: 8min 8s\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_panamahmean_sum, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9881889763779528"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_w2v_panamahmean_sum, y_validation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors extracted from pre-trained GloVe (Average/Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim.downloader as api\n",
    "glove_twitter = {}\n",
    "GLOVE_TWITTER27B_200D_PATH = \"../datasets/combined/data/glove.twitter.27B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(GLOVE_TWITTER27B_200D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding='utf8')\n",
    "        #if (word in all_words):\n",
    "        nums=np.array(parts[1:], dtype=np.float32)\n",
    "        glove_twitter[word] = nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in x_train]))\n",
    "validation_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 31s, sys: 316 ms, total: 7min 31s\n",
      "Wall time: 7min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_glove_mean, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9251968503937008"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_glove_mean, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_glove_sum = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'sum') for z in x_train]))\n",
    "validation_vecs_glove_sum = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'sum') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 58s, sys: 420 ms, total: 7min 58s\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_glove_sum, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9292374637380854"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_glove_sum, y_validation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors extracted from pre-trained Google News Word2Vec (Average/Sum)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import gensim.downloader as api\n",
    "googlenews = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_googlenews_mean = scale(np.concatenate([get_w2v_general(z, 300, googlenews,'mean') for z in x_train]))\n",
    "validation_vecs_googlenews_mean = scale(np.concatenate([get_w2v_general(z, 300, googlenews,'mean') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 55s, sys: 41min 5s, total: 47min\n",
      "Wall time: 1h 23min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_googlenews_mean, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.749561403508772"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_googlenews_mean, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_googlenews_sum = scale(np.concatenate([get_w2v_general(z, 300, googlenews,'sum') for z in x_train]))\n",
    "validation_vecs_googlenews_sum = scale(np.concatenate([get_w2v_general(z, 300, googlenews,'sum') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 45s, sys: 39min 17s, total: 45min 2s\n",
      "Wall time: 1h 19min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_googlenews_sum, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7491854636591478"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_googlenews_sum, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separately trained Word2Vec (Average/Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/all_x_w2v.p', 'wb') as fp:\n",
    "    pickle.dump(all_x_w2v, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/all_x_w2v.p', 'rb') as fp:\n",
    "    all_x_w2v = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 965236/965236 [00:00<00:00, 2080003.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.38 s, sys: 24 ms, total: 6.4 s\n",
      "Wall time: 8.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 965236/965236 [00:00<00:00, 2337274.24it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2549170.53it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2540557.82it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2453987.71it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2549401.69it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2286518.25it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2489788.51it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2398419.19it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2531198.93it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2487217.22it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2489254.23it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2497323.30it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2374825.47it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2477583.73it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2523875.53it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2466923.74it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2454926.67it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2539509.22it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2548092.35it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2541856.23it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2486377.08it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2464822.55it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2405882.05it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2240290.10it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2479840.41it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2431378.51it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2297046.10it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2443206.42it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2499100.74it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2505900.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 30s, sys: 3.17 s, total: 12min 33s\n",
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_cbow.alpha -= 0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 57s, sys: 320 ms, total: 2min 57s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_cbow_mean = scale(np.concatenate([get_w2v_general(z, 100, model_ug_cbow,'mean') for z in x_train]))\n",
    "validation_vecs_cbow_mean = scale(np.concatenate([get_w2v_general(z, 100, model_ug_cbow,'mean') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 12s, sys: 28 ms, total: 4min 12s\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbow_mean, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814546208039785"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_cbow_mean, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ug_cbow.save('../datasets/combined/models/classification/d2v_model_ug_cbow.word2vec')\n",
    "model_ug_cbow = Word2Vec.load('../datasets/combined/models/classification/d2v_model_ug_cbow.word2vec')\n",
    "#model_ug_cbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 965236/965236 [00:00<00:00, 2138060.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 965236/965236 [00:00<00:00, 2391669.18it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2371586.85it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2453532.62it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2274958.40it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 1774095.58it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2363290.12it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2495186.94it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2474033.28it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2397805.53it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2466940.27it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2394274.56it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2497383.38it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2385342.08it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2361076.62it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2474650.28it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2487955.48it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2396733.79it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2125977.84it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2460729.59it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2008376.42it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2444782.14it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2441301.46it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2509720.71it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2239410.27it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2431038.33it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2481853.18it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2456112.18it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2469358.32it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2324849.67it/s]\n",
      "100%|██████████| 965236/965236 [00:00<00:00, 2489479.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 21s, sys: 3.76 s, total: 24min 25s\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_sg.alpha -= 0.002\n",
    "    model_ug_sg.min_alpha = model_ug_sg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_vecs_sg_mean = scale(np.concatenate([get_w2v_general(z, 100, model_ug_sg,'mean') for z in x_train]))\n",
    "validation_vecs_sg_mean = scale(np.concatenate([get_w2v_general(z, 100, model_ug_sg,'mean') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 60 ms, total: 2min 52s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_sg_mean, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814546208039785"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_sg_mean, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ug_sg.save('../datasets/combined/models/classification/d2v_model_ug_sg.word2vec')\n",
    "model_ug_sg = Word2Vec.load('../datasets/combined/models/classification/d2v_model_ug_sg.word2vec')\n",
    "#model_ug_sg.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_mean(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 59s, sys: 6.28 s, total: 6min 5s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_cbowsg_mean = scale(np.concatenate([get_w2v_mean(z, 200) for z in x_train]))\n",
    "validation_vecs_cbowsg_mean = scale(np.concatenate([get_w2v_mean(z, 200) for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 53s, sys: 496 ms, total: 10min 54s\n",
      "Wall time: 10min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbowsg_mean, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/clf_lr_cbowsg_mean.p', 'wb') as fp:\n",
    "    pickle.dump(clf, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/clf_lr_cbowsg_mean.p', 'rb') as fp:\n",
    "    clf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9871529216742644"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_cbowsg_mean, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_sum(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1, size))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train_vecs_cbowsg_sum = scale(np.concatenate([get_w2v_sum(z, 200) for z in x_train]))\n",
    "validation_vecs_cbowsg_sum = scale(np.concatenate([get_w2v_sum(z, 200) for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 28s, sys: 380 ms, total: 11min 28s\n",
      "Wall time: 11min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbowsg_sum, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/clf_lr_cbowsg_sum.p', 'wb') as fp:\n",
    "    pickle.dump(clf, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/clf_lr_cbowsg_sum.p', 'rb') as fp:\n",
    "    clf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986738499792789"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_cbowsg_sum, y_validation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separately trained Word2Vec with custom weighting (Average/Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizeb/.local/lib/python3.5/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.92 s, sys: 24 ms, total: 4.94 s\n",
      "Wall time: 4.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_panama_hmean_01 = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    if w in panama_hmean.keys():\n",
    "        w2v_panama_hmean_01[w] = np.append(model_ug_cbow[w],model_ug_sg[w]) * panama_hmean[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 1.43 s, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vecs_w2v_panamahmean_mean_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_panama_hmean_01, 'mean') for z in x_train]))\n",
    "validation_vecs_w2v_panamahmean_mean_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_panama_hmean_01, 'mean') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 18s, sys: 372 ms, total: 12min 19s\n",
      "Wall time: 12min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_panamahmean_mean_01, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/clf_lr_w2v_panamahmean_mean.p', 'wb') as fp:\n",
    "    pickle.dump(clf, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/clf_lr_w2v_panamahmean_mean.p', 'rb') as fp:\n",
    "    clf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98487360132615"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_w2v_panamahmean_mean_01, y_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_w2v_panamahmean_sum_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_panama_hmean_01, 'sum') for z in x_train]))\n",
    "validation_vecs_w2v_panamahmean_sum_01 = scale(np.concatenate([get_w2v_general(z, 200, w2v_panama_hmean_01, 'sum') for z in x_validation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 24s, sys: 796 ms, total: 11min 25s\n",
      "Wall time: 11min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_w2v_panamahmean_sum_01, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/combined/models/classification/clf_lr_w2v_panamahmean_sum_01.p', 'wb') as fp:\n",
    "    pickle.dump(clf, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859096560298384"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_w2v_panamahmean_sum_01, y_validation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v_final = train_vecs_w2v_panamahmean_mean_01\n",
    "validation_w2v_final = validation_vecs_w2v_panamahmean_mean_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 945931 samples, validate on 9652 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.99529, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.00-0.9953.hdf5\n",
      "119s - loss: 0.0155 - acc: 0.9949 - val_loss: 0.0139 - val_acc: 0.9953\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.99529 to 0.99629, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.01-0.9963.hdf5\n",
      "105s - loss: 0.0112 - acc: 0.9963 - val_loss: 0.0114 - val_acc: 0.9963\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.99629 to 0.99655, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.02-0.9965.hdf5\n",
      "106s - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0107 - val_acc: 0.9965\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc did not improve\n",
      "109s - loss: 0.0091 - acc: 0.9969 - val_loss: 0.0111 - val_acc: 0.9961\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc did not improve\n",
      "112s - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0113 - val_acc: 0.9964\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc improved from 0.99655 to 0.99656, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.05-0.9966.hdf5\n",
      "107s - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0102 - val_acc: 0.9966\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc improved from 0.99656 to 0.99658, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.06-0.9966.hdf5\n",
      "108s - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0100 - val_acc: 0.9966\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc did not improve\n",
      "106s - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0113 - val_acc: 0.9966\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc improved from 0.99658 to 0.99663, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.08-0.9966.hdf5\n",
      "106s - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0102 - val_acc: 0.9966\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc improved from 0.99663 to 0.99675, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.09-0.9968.hdf5\n",
      "106s - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0107 - val_acc: 0.9968\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "106s - loss: 0.0067 - acc: 0.9978 - val_loss: 0.0142 - val_acc: 0.9963\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc did not improve\n",
      "106s - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0119 - val_acc: 0.9967\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc did not improve\n",
      "106s - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0114 - val_acc: 0.9964\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc improved from 0.99675 to 0.99679, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.13-0.9968.hdf5\n",
      "106s - loss: 0.0065 - acc: 0.9980 - val_loss: 0.0115 - val_acc: 0.9968\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc did not improve\n",
      "106s - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0129 - val_acc: 0.9965\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "106s - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0132 - val_acc: 0.9963\n",
      "Epoch 17/100\n",
      "Epoch 00016: val_acc did not improve\n",
      "106s - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0134 - val_acc: 0.9966\n",
      "Epoch 18/100\n",
      "Epoch 00017: val_acc improved from 0.99679 to 0.99684, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.17-0.9968.hdf5\n",
      "106s - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0122 - val_acc: 0.9968\n",
      "Epoch 19/100\n",
      "Epoch 00018: val_acc improved from 0.99684 to 0.99694, saving model to ../datasets/combined/models/classification/w2v_01_best_weights.18-0.9969.hdf5\n",
      "106s - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0126 - val_acc: 0.9969\n",
      "Epoch 20/100\n",
      "Epoch 00019: val_acc did not improve\n",
      "106s - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0143 - val_acc: 0.9967\n",
      "Epoch 21/100\n",
      "Epoch 00020: val_acc did not improve\n",
      "106s - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0174 - val_acc: 0.9966\n",
      "Epoch 22/100\n",
      "Epoch 00021: val_acc did not improve\n",
      "106s - loss: 0.0070 - acc: 0.9981 - val_loss: 0.0134 - val_acc: 0.9965\n",
      "Epoch 23/100\n",
      "Epoch 00022: val_acc did not improve\n",
      "106s - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0172 - val_acc: 0.9964\n",
      "Epoch 24/100\n",
      "Epoch 00023: val_acc did not improve\n",
      "106s - loss: 0.0063 - acc: 0.9982 - val_loss: 0.0167 - val_acc: 0.9964\n",
      "Epoch 25/100\n",
      "Epoch 00024: val_acc did not improve\n",
      "106s - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0150 - val_acc: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2d4bf69b0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "filepath=\"../datasets/combined/models/classification/w2v_01_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=5, mode='max') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "np.random.seed(seed)\n",
    "model_w2v_01 = Sequential()\n",
    "model_w2v_01.add(Dense(256, activation='relu', input_dim=200))\n",
    "model_w2v_01.add(Dense(256, activation='relu'))\n",
    "model_w2v_01.add(Dense(256, activation='relu'))\n",
    "model_w2v_01.add(Dense(nb_classes, activation='sigmoid'))\n",
    "model_w2v_01.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_w2v_01.fit(train_w2v_final, y_train, validation_data=(validation_w2v_final, y_validation), \n",
    "                 epochs=100, batch_size=32, verbose=2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_w2v_model = load_model('../datasets/combined/models/classification/w2v_01_best_weights.10-0.8048.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15960/15960 [==============================] - 0s 20us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4244666022615026, 0.8047619047619048]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_w2v_model.evaluate(x=validation_w2v_final, y=y_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
